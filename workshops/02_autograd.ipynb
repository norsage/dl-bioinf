{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Autograd в PyTorch\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При обучении нейронных сетей мы минимизируем значение функции ошибки на обучающей выборке, меняя значения параметров модели. Чтобы понять, куда нужно сместить значения параметров, нужно уметь считать градиент — именно для автоматизации этих расчётов нам и нужен Pytorch. Посмотрим, как именно он это делает."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но перед этим вспомним правило дифференцирования сложной функции:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\frac{\\partial f}{\\partial x} =\n",
    "\\frac{\\partial f}{\\partial u} \\cdot \n",
    "\\frac{\\partial u}{\\partial x}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Пример:\n",
    "\n",
    "$$\n",
    "f(x) = \\sin(\\ln x) \\quad\n",
    "u(x) = \\ln x  \\quad\n",
    "f(u) = \\sin(u)$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} =\n",
    "\\frac{\\partial f}{\\partial u} \\cdot \n",
    "\\frac{\\partial u}{\\partial x} = \n",
    "\\cos(u) \\cdot \n",
    "\\frac{1}{x} = \n",
    "\\cos(\\ln x) \\cdot \n",
    "\\frac{1}{x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Дифференцирование и вычислительный граф"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим выражение $f(x, y) = x^2 + xy + (x + y)^2$ и построим его граф:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img height=500 src=\"../assets/forward_pass.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Производная по переменной $x$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial f}{\\partial x} &=\n",
    "\\color{violet}\n",
    "\\frac{\\partial f}{\\partial d} \\cdot \n",
    "\\frac{\\partial d}{\\partial a} \\cdot \n",
    "\\frac{\\partial a}{\\partial x}\n",
    "\\color{white}\n",
    "+\n",
    "\\color{green}\n",
    "\\frac{\\partial f}{\\partial d} \\cdot \n",
    "\\frac{\\partial d}{\\partial b} \\cdot \n",
    "\\frac{\\partial b}{\\partial x}\n",
    "\\color{white}\n",
    "+\n",
    "\\color{cyan}\n",
    "\\frac{\\partial f}{\\partial e} \\cdot \n",
    "\\frac{\\partial e}{\\partial c} \\cdot \n",
    "\\frac{\\partial c}{\\partial x} \\\\\n",
    "&=\n",
    "\\color{violet}\n",
    "1 \\cdot 1 \\cdot 2x\n",
    "\\color{white}\n",
    "+\n",
    "\\color{green}\n",
    "1 \\cdot 1 \\cdot y\n",
    "\\color{white}\n",
    "+\n",
    "\\color{cyan}\n",
    "1 \\cdot 2c \\cdot 1\n",
    "\\color{white}\n",
    "=\n",
    "2x + y + 2(x+y)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для вычисления производных Pytorch строит вычислительный граф, проход по которому позволяет рассчитать градиенты по правилу производной сложной функции (chain rule).\n",
    "\n",
    "Прямой проход:\n",
    "- расчёт значения выходного тензора\n",
    "- построение графа и сохранение нужных для обратного прохода данных для каждой операции\n",
    "\n",
    "Обратный проход (вызов `.backward()` у корня графа):\n",
    "- расчёт градиентов и их накопление в артибуте `.grad` каждого тензора\n",
    "- распространение вычислений далее до листьев графа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем выражение для $f(x, y)$, задав начальные условия $x = 2.0, y = 2.0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(24., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(2.0, requires_grad=True)\n",
    "a = x**2\n",
    "b = x * y\n",
    "c = x + y\n",
    "d = a + b\n",
    "e = c**2\n",
    "f = d + e\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`grad_fn` означает, что `f` не просто отдельный тензор, а связан с вычислительным графом и соответствует операции `Add`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим backprop и убедимся, что градиенты рассчитаны правильно:\n",
    "\n",
    "$\\frac{\\partial f}{\\partial x} = 2x + y + 2(x + y)$\n",
    "\n",
    "$\\frac{\\partial f}{\\partial y} = x + 2(x + y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14.)\n",
      "tensor(10.)\n"
     ]
    }
   ],
   "source": [
    "f.backward()\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Повторный вызов `backward()` приведёт к ошибке, потому что после предыдущего вызова граф уже уничтожен для высвобождения ресурсов. Такое поведение по умолчанию оправдано, но если мы по какой-то причине хотим сохранить граф, следует указать `.backward(retain_graph=True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первый вызов\n",
      "Производная по x:  tensor(14.)\n",
      "Производная по x:  tensor(10.)\n",
      "\n",
      "Второй вызов\n",
      "Производная по x:  tensor(28.)\n",
      "Производная по x:  tensor(20.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "f = x**2 + x * y + (x + y)**2\n",
    "\n",
    "# вызовем backward дважды и посмотрим на градиенты:\n",
    "print(\"Первый вызов\")\n",
    "f.backward(retain_graph=True)\n",
    "print(\"Производная по x: \", x.grad)\n",
    "print(\"Производная по x: \", y.grad)\n",
    "\n",
    "print(\"\\nВторой вызов\")\n",
    "f.backward(retain_graph=True)\n",
    "print(\"Производная по x: \", x.grad)\n",
    "print(\"Производная по x: \", y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После второго вызова градиенты удвоились, но здесь нет ошибки: градиенты накапливаются в поле `.grad`, и если мы хотим избавиться от истории прошлых вычислений, это стоит сделать явно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первый вызов\n",
      "Производная по x:  tensor(14.)\n",
      "Производная по x:  tensor(10.)\n",
      "\n",
      "Второй вызов\n",
      "Производная по x:  tensor(14.)\n",
      "Производная по x:  tensor(10.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(2.0, requires_grad=True)\n",
    "a = x**2\n",
    "b = x * y\n",
    "c = x + y\n",
    "d = a + b\n",
    "e = c**2\n",
    "f = d + e\n",
    "\n",
    "# вызовем backward дважды и посмотрим на градиенты:\n",
    "print(\"Первый вызов\")\n",
    "f.backward(retain_graph=True)\n",
    "print(\"Производная по x: \", x.grad)\n",
    "print(\"Производная по x: \", y.grad)\n",
    "# обнулим градиенты, можно сделать двумя способами\n",
    "x.grad = None\n",
    "y.grad = torch.tensor(0.0)\n",
    "\n",
    "print(\"\\nВторой вызов\")\n",
    "f.backward(retain_graph=True)\n",
    "print(\"Производная по x: \", x.grad)\n",
    "print(\"Производная по x: \", y.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
