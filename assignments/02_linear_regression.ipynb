{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Линейная регрессия на `pytorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании мы попробуем решить задачу линейно регрессии, где наша целевая переменная будет зависеть от двух признаков:\n",
    "\n",
    "$$y = w_1 x_1 + w_2 x_2 + b$$\n",
    "\n",
    "Мы сгенерируем датасет и попробуем на его основе узнать коэффициенты регрессии несколькими способами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Математическая справка\n",
    "\n",
    "***Дисклеймер***: *для решения задания нам потребуются только две формулы, выделенные цветом, справка дана исключительно для того, чтобы вы при желании могли разобраться, откуда эти формулы возникают.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Переход к матричному виду\n",
    "\n",
    "Значения коэффициентов линейной регрессии можно вычислить аналитически. Чтобы описать необходимое вычисление более компактно, давайте всё переведём в матричный вид, чтобы вычисление предсказаний модели выглядело так:\n",
    "$$\\mathbf{\\hat{y}} = \\mathbf{X} \\mathbf{w}$$\n",
    "\n",
    "где $\\mathbf{\\hat{y}}$ — это вектор со предсказаниями целевой переменной для всех наблюдений, $\\mathbf{X}$ — матрица со значениями всех признаков для наших наблюдений, а $\\mathbf{w}$ — вектор с параметрами линейной модели, его нам и нужно будет найти:\n",
    "\n",
    "$$\\mathbf{w} =\n",
    "\\begin{pmatrix}\n",
    "b \\\\\n",
    "w_1 \\\\\n",
    "w_2\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Но в нашей модели три параметра, а признаков два! Поэтому, чтобы размерности параметров и признаков сошлись, мы ко всем наблюдениям добавим фиктивный признак, значение которого равно 1 — это стандартный трюк, позволяющий записывать линейные модели без явного свободного параметра $b$ в уравнениях.\n",
    "\n",
    "Проверим, что мы ничего не сломали, и модель для всех наблюдений по-прежнему считается так, как задумано.\n",
    "\n",
    "Вспоминаем, что при умножении матрицы $\\mathbf{X}$ на вектор $\\mathbf{w}$ мы получаем новый вектор, в $i$-м элементе которого будет результат скалярного произведения (т.е. суммы поэлементных произведений) $i$-й строки матрицы $\\mathbf{X}$ на вектор $\\mathbf{w}$:\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{\\hat{y}}[i] = \\mathbf{X}[i] \\cdot \\mathbf{w} = (1, x_1^{(i)}, x_2^{(i)}) \\cdot \n",
    "\\begin{pmatrix}\n",
    "b \\\\\n",
    "w_1 \\\\\n",
    "w_2\n",
    "\\end{pmatrix}\n",
    " = 1 \\cdot b + x_1^{(i)} \\cdot w_1 + x_2^{(i)} \\cdot w_2\n",
    "$$\n",
    "\n",
    "То есть для всех наших наблюдений формула, получающая $\\hat{y}$ по значениям параметров и признаков осталась той же, но у нас теперь есть удобная матричная запись.\n",
    "\n",
    "$$\n",
    "\\color{violet}\n",
    "\\mathbf{\\hat{y}} = \\mathbf{X} \\mathbf{w}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Функция ошибки\n",
    "\n",
    "Теперь мы хотим, чтобы наши предсказания были близки к верным значениям $\\mathbf{y}$. Наше желание можно математически описать так: пусть суммарные квадраты ошибок предсказаний будут минимальны:\n",
    "\n",
    "$$\n",
    "\\color{violet}\n",
    "E(\\mathbf{w}) = \\sum_{i} (\\mathbf{y}_i - \\mathbf{\\hat{y}}_i)^2 = \\sum_{i} (\\mathbf{y}_i - \\mathbf{X}[i] \\cdot \\mathbf{w})^2  \\rightarrow \\min\n",
    "$$\n",
    "\n",
    "В матричной записи это будет выглядеть так:\n",
    "\n",
    "$$\n",
    "\\color{violet}\n",
    "E(\\mathbf{w}) = (\\mathbf{y} - \\mathbf{X} \\mathbf{w})^T (\\mathbf{y} - \\mathbf{X} \\mathbf{w}) \\rightarrow \\min\n",
    "$$\n",
    "\n",
    "**Поясним за нотацию**: *$(\\mathbf{y} - \\mathbf{X} \\mathbf{w})^T$ означает, что мы транспонируем вектор $(\\mathbf{y} - \\mathbf{X} \\mathbf{w})$, то есть рассматриваем его как вектор-строку, а не вектор-столбец.*\n",
    "\n",
    "\n",
    "То есть мы считаем ошибки для всех наблюдений, складываем их в вектор, и считаем его скалярное произведение с самим собой.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Градиент и аналитическое решение\n",
    "\n",
    "Чтобы найти локальный минимум этой функции, мы можем посчитать градиент нашей функции ошибки по параметрам:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E(\\mathbf{w})}{\\partial \\mathbf{w}}\n",
    "= -2 \\mathbf{X}^T (\\mathbf{y} - \\mathbf{X} \\mathbf{w})\n",
    "$$\n",
    "\n",
    "Это нам пригодится, когда мы будем искать решение градиентным спуском.\n",
    "\n",
    "***Замечание***: *двойка перед скобкой не влияет на направление градиента, и для удобства её часто опускают. Чтобы не терять математической строгости, именно поэтому функция ошибки часто включает множитель $\\frac{1}{2}$, чтобы потом при взятии производных $2$ и $\\frac{1}{2}$ взаимно сократились:*\n",
    "\n",
    "\n",
    "$$\n",
    "E(\\mathbf{w}) = \\frac{1}{2} (\\mathbf{y} - \\mathbf{X} \\mathbf{w})^T (\\mathbf{y} - \\mathbf{X} \\mathbf{w}) \\rightarrow \\min\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\color{green}\n",
    "\\frac{\\partial E(\\mathbf{w})}{\\partial \\mathbf{w}} = \\nabla_\\mathbf{w} E(\\mathbf{w})\n",
    "= - \\mathbf{X}^T (\\mathbf{y} - \\mathbf{X} \\mathbf{w})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но оказывается, что для нашей модели значение параметров $\\mathbf{w}$, которые минимизируют эту ошибку, можно найти за один шаг!\n",
    "У нашей функции ошибки единственный локальный минимум (значит, он же и глобальный), а в нём градиент должен быть равен нулю. Значит нужно решить уравнение:\n",
    "$$\n",
    "% \\begin{align*}\n",
    "\\nabla_\\mathbf{w} E(\\mathbf{w})\n",
    "= -\\mathbf{X}^T (\\mathbf{y} - \\mathbf{X} \\mathbf{w}) = \\mathbf{0}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{X}^T \\mathbf{X} \\mathbf{w} = \\mathbf{X}^T \\mathbf{y} \\\\\n",
    "$$\n",
    "$$\n",
    "(\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{X} \\mathbf{w} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\color{red}\n",
    "\\mathbf{w} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n",
    "$$\n",
    "\n",
    "**Поясним за нотацию**: *$(\\mathbf{X}^T \\mathbf{X})^{-1}$ — это обратная матрица от матричного произведения $\\mathbf{X}^T$ на $\\mathbf{X}$*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Смоделируем наши данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# фиксируем генератор случайных чисел\n",
    "torch.manual_seed(42)\n",
    "\n",
    "n_observations = 64\n",
    "\n",
    "x1 = torch.randn(n_observations)\n",
    "x2 = torch.randn(n_observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смоделируем $y = 2x_1 - 3x_2 + 1 + \\varepsilon$, где $\\varepsilon \\sim \\mathcal{N}(0, 1)$ — небольшой шум\n",
    "\n",
    "То есть наши истинные значения параметров:\n",
    "\n",
    "$$\\mathbf{w} =\n",
    "\\begin{pmatrix}\n",
    "b \\\\\n",
    "w_1 \\\\\n",
    "w_2\n",
    "\\end{pmatrix}\n",
    " =\n",
    "\\begin{pmatrix}\n",
    "\\phantom{-}1 \\\\\n",
    "\\phantom{-}2 \\\\\n",
    "-3\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = torch.randn(n_observations)\n",
    "\n",
    "y = 2*x1 - 3*x2 + 1 + eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можем посмотреть на наши данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=x1,\n",
    "    y=x2,\n",
    "    z=y,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=12,\n",
    "        color=y,                # set color to an array/list of desired values\n",
    "        colorscale='Viridis',   # choose a colorscale\n",
    "        opacity=0.8\n",
    "    ),\n",
    "    hovertemplate=\"x1=%{x:.2f}, x2=%{y:.2f}, y=%{z:.2f}\",\n",
    ")])\n",
    "\n",
    "# tight layout\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title=\"x1\",\n",
    "        yaxis_title=\"x2\",\n",
    "        zaxis_title=\"y\",\n",
    "        aspectratio=dict(x=1, y=1, z=1)\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 1. Аналитическое решение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посчитаем точное решение с помощью Pytorch\n",
    "\n",
    "$$\n",
    "\\color{red}\n",
    "\\mathbf{w} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n",
    "$$\n",
    "\n",
    "Вам понадобится:\n",
    "1. Функция, которая сможет склеивать векторы отдельных признаков в единую матрицу. В этот раз нам подойдёт функция `torch.stack`, но посмотрите также на функцию `torch.cat`, они делают похожие вещи, но немного по-разному.\n",
    "2. Операция матричного умножения — это функция `torch.matmul` или оператор `@`, его мы уже видели\n",
    "3. Операция транспонирования $X^T$ — через атрибут `T` тензора.\n",
    "4. Функция для расчёта обратной матрицы — `torch.linalg.inv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_feature = torch.ones_like(x1)\n",
    "\n",
    "X = torch.stack(...)  # объедините dummy_feature, x1 и x2 в один тензор признаков, его форма: [64, 3]\n",
    "\n",
    "w_exact = ...  # посчитайте параметры модели по красной формуле\n",
    "w_exact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 2. Градиентный спуск вручную"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь будем решать задачу градиентным спуском — это будет наш главный способ обучать более сложные модели.\n",
    "\n",
    "На каждой итерации мы будем:\n",
    "1. Оценивать градиент $\\color{green} \\nabla_\\mathbf{w} E(\\mathbf{w}) = -\\mathbf{X}^T (\\mathbf{y} - \\mathbf{X} \\mathbf{w})$\n",
    "2. Шагать в противоположную сторону с небольшим шагом $\\eta$: $\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\eta \\cdot \\nabla_\\mathbf{w} E(\\mathbf{w})$\n",
    "\n",
    "Нам потребуется определить:\n",
    "- начальное значение $\\mathbf{w}$ — выберем из стандартного нормального распределения\n",
    "- размер шага обновления $\\eta$\n",
    "- количество шагов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.randn(3)  # начальное значение\n",
    "learning_rate = 0.005  # шаг оптимизации\n",
    "n_steps = 100  # число итераций\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_steps):\n",
    "    grad = ...  # посчитайте по зелёной формуле\n",
    "    w -= learning_rate * grad\n",
    "    if (i+1) % 10 == 0:\n",
    "        print(f\"step {i+1}: w = {w}, gradient = {grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 3. Автоматический градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эту сторону pytorch мы с вами ещё подробно не смотрели, но давайте немного забежим вперёд и просто посмотрим, чем он нам так полезен.\n",
    "\n",
    "Выше в ручном градиентном спуске при расчёте градиентов мы пользовались готовой формулой для линейной регрессии, которую старательно выводили в математической справке.\n",
    "\n",
    "Но мы точно не хотели бы делать что-то подобное для глубоких нейронных сетей: выкладки были бы очень громоздкими.\n",
    "В лекции мы говорили, что pytorch умеет считать градиенты самостоятельно. Давайте посмотрим, как мы будем этим пользоваться."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Первое отличие**: тензор с нашими параметрами мы помечаем как `requires_grad=True`; так pytorch будет знать, что этот тензор требует расчёта градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.randn(3, requires_grad=True)  # начальное значение\n",
    "learning_rate = 0.005  # шаг оптимизации\n",
    "n_steps = 100  # число итераций\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Второе отличие**: теперь вместо явной формулы для расчёта градиентов нам просто нужно получить вычисленное значение ошибки при текущих значениях параметров, которое мы сохраним в переменную `loss`. Затем мы вызовем у неё метод `.backward`, чтобы рассчитались градиенты для всех тензоров, которые участвуют в вычислении ошибки и помечены как `requires_grad=True`; в нашем случае это только вектор параметров `w`.\n",
    "\n",
    "Посчитанные градиенты попадут в атрибут `grad` нашего тензора, и далее мы используем их, чтобы обновить значения параметров и перейти к следующей итерации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вам в этом задании нужно сделать две вещи:\n",
    "- Получить предсказания модели `y_hat` из признаков `X` и текущих значений параметров `w`: $$\\color{violet}\\mathbf{\\hat{y}} = \\mathbf{X} \\mathbf{w}$$\n",
    "- Рассчитать значение ошибки из предсказаний `y_hat` и целевой переменной `y`: $$\\color{violet} E(\\mathbf{w}) = \\sum_{i} (\\mathbf{y}_i - \\mathbf{\\hat{y}}_i)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_steps):\n",
    "    # прямой проход\n",
    "    y_hat = ...  # предсказания модели на основе X и w\n",
    "    loss = ...  # значение ошибки\n",
    "\n",
    "    # обратный проход\n",
    "    loss.backward()  # рассчитываем градиенты\n",
    "\n",
    "    # обновляем параметры, локально отключая расчёт градиентов, чтобы не запутать pytorch\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f\"step {i+1}: w = {w}, gradient = {w.grad}\")\n",
    "\n",
    "    # обнуляем градиенты, чтобы они не суммировались накопленным итогом\n",
    "    w.grad = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
